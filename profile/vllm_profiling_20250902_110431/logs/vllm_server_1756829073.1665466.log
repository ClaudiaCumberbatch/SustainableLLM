INFO 09-02 11:04:37 [__init__.py:241] Automatically detected platform cuda.
WARNING 09-02 11:04:38 [__init__.py:1726] argument '--disable-log-requests' is deprecated and replaced with '--enable-log-requests'. This will be removed in v0.12.0.
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:04:38 [api_server.py:1805] vLLM API server version 0.10.1.1
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:04:38 [utils.py:326] non-default args: {'model': 'meta-llama/Llama-2-7b-hf', 'max_model_len': 512, 'enforce_eager': True, 'enable_prefix_caching': True, 'max_num_batched_tokens': 4096, 'max_num_seqs': 128}
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:04:45 [__init__.py:711] Resolved architecture: LlamaForCausalLM
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:04:45 [__init__.py:1750] Using max model len 512
[1;36m(APIServer pid=637228)[0;0m WARNING 09-02 11:04:45 [arg_utils.py:1770] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. 
[1;36m(APIServer pid=637228)[0;0m WARNING 09-02 11:04:47 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:04:47 [__init__.py:3565] Cudagraph is disabled under eager mode
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:04:47 [api_server.py:295] Started engine process with PID 637406
INFO 09-02 11:04:51 [__init__.py:241] Automatically detected platform cuda.
INFO 09-02 11:04:52 [llm_engine.py:222] Initializing a V0 LLM engine (v0.10.1.1) with config: model='meta-llama/Llama-2-7b-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=512, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-2-7b-hf, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{"enable_fusion":false,"enable_noop":false},"max_capture_size":0,"local_cache_dir":null}, use_cached_outputs=True, 
INFO 09-02 11:04:53 [cuda.py:384] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 09-02 11:04:53 [cuda.py:433] Using XFormers backend.
INFO 09-02 11:04:54 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 09-02 11:04:54 [model_runner.py:1080] Starting to load model meta-llama/Llama-2-7b-hf...
INFO 09-02 11:04:55 [weight_utils.py:296] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.65it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.27s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.17s/it]

INFO 09-02 11:04:57 [default_loader.py:262] Loading weights took 2.54 seconds
INFO 09-02 11:04:58 [model_runner.py:1112] Model loading took 12.5524 GiB and 2.944934 seconds
INFO 09-02 11:04:59 [worker.py:295] Memory profiling takes 1.39 seconds
INFO 09-02 11:04:59 [worker.py:295] the current vLLM instance can use total_gpu_memory (15.77GiB) x gpu_memory_utilization (0.90) = 14.20GiB
INFO 09-02 11:04:59 [worker.py:295] model weights take 12.55GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 0.35GiB; the rest of the memory reserved for KV Cache is 1.22GiB.
INFO 09-02 11:04:59 [executor_base.py:114] # cuda blocks: 156, # CPU blocks: 512
INFO 09-02 11:04:59 [executor_base.py:119] Maximum concurrency for 512 tokens per request: 4.88x
INFO 09-02 11:05:01 [llm_engine.py:417] init engine (profile, create kv cache, warmup model) took 3.65 seconds
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [api_server.py:1611] Supported_tasks: ['generate']
[1;36m(APIServer pid=637228)[0;0m WARNING 09-02 11:05:03 [__init__.py:1625] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [serving_responses.py:120] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [serving_chat.py:134] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [serving_completion.py:77] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [api_server.py:1880] Starting vLLM API server 0 on http://0.0.0.0:8000
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:36] Available routes are:
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /openapi.json, Methods: GET, HEAD
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /docs, Methods: GET, HEAD
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /redoc, Methods: GET, HEAD
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /health, Methods: GET
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /load, Methods: GET
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /ping, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /ping, Methods: GET
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /version, Methods: GET
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /pooling, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /classify, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /score, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /rerank, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /invocations, Methods: POST
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:05:03 [launcher.py:44] Route: /metrics, Methods: GET
[1;36m(APIServer pid=637228)[0;0m INFO:     Started server process [637228]
[1;36m(APIServer pid=637228)[0;0m INFO:     Waiting for application startup.
[1;36m(APIServer pid=637228)[0;0m INFO:     Application startup complete.
[1;36m(APIServer pid=637228)[0;0m INFO:     127.0.0.1:48926 - "GET /health HTTP/1.1" 200 OK
[1;36m(APIServer pid=637228)[0;0m INFO:     127.0.0.1:48942 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=637228)[0;0m INFO:     127.0.0.1:48956 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 09-02 11:05:08 [metrics.py:386] Avg prompt throughput: 14.4 tokens/s, Avg generation throughput: 30.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.
INFO 09-02 11:05:08 [metrics.py:402] Prefix cache hit rate: GPU: 0.00%, CPU: 0.00%
[1;36m(APIServer pid=637228)[0;0m INFO:     127.0.0.1:54022 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=637228)[0;0m INFO:     127.0.0.1:54038 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=637228)[0;0m INFO:     127.0.0.1:54044 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 09-02 11:05:13 [metrics.py:386] Avg prompt throughput: 48.0 tokens/s, Avg generation throughput: 48.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.
INFO 09-02 11:05:13 [metrics.py:402] Prefix cache hit rate: GPU: 0.00%, CPU: 0.00%
INFO 09-02 11:05:25 [metrics.py:386] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 09-02 11:05:25 [metrics.py:402] Prefix cache hit rate: GPU: 0.00%, CPU: 0.00%
INFO 09-02 11:05:35 [metrics.py:386] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 09-02 11:05:35 [metrics.py:402] Prefix cache hit rate: GPU: 0.00%, CPU: 0.00%
[1;36m(APIServer pid=637228)[0;0m INFO:     127.0.0.1:52108 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 09-02 11:06:18 [metrics.py:386] Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 09-02 11:06:18 [metrics.py:402] Prefix cache hit rate: GPU: 0.00%, CPU: 0.00%
[1;36m(APIServer pid=637228)[0;0m INFO:     127.0.0.1:54192 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 09-02 11:06:26 [metrics.py:386] Avg prompt throughput: 10.3 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.
INFO 09-02 11:06:26 [metrics.py:402] Prefix cache hit rate: GPU: 0.00%, CPU: 0.00%
INFO 09-02 11:06:38 [metrics.py:386] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 09-02 11:06:38 [metrics.py:402] Prefix cache hit rate: GPU: 0.00%, CPU: 0.00%
INFO 09-02 11:06:48 [metrics.py:386] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 09-02 11:06:48 [metrics.py:402] Prefix cache hit rate: GPU: 0.00%, CPU: 0.00%
[1;36m(APIServer pid=637228)[0;0m INFO:     127.0.0.1:42686 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 09-02 11:07:30 [metrics.py:386] Avg prompt throughput: 3.7 tokens/s, Avg generation throughput: 0.9 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 09-02 11:07:30 [metrics.py:402] Prefix cache hit rate: GPU: 0.00%, CPU: 0.00%
INFO 09-02 11:07:40 [metrics.py:386] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 09-02 11:07:40 [metrics.py:402] Prefix cache hit rate: GPU: 0.00%, CPU: 0.00%
[1;36m(APIServer pid=637228)[0;0m INFO:     127.0.0.1:50636 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 09-02 11:08:37 [metrics.py:386] Avg prompt throughput: 8.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 09-02 11:08:37 [metrics.py:402] Prefix cache hit rate: GPU: 0.00%, CPU: 0.00%
INFO 09-02 11:08:49 [metrics.py:386] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 09-02 11:08:49 [metrics.py:402] Prefix cache hit rate: GPU: 0.00%, CPU: 0.00%
INFO 09-02 11:08:59 [metrics.py:386] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 09-02 11:08:59 [metrics.py:402] Prefix cache hit rate: GPU: 0.00%, CPU: 0.00%
[1;36m(APIServer pid=637228)[0;0m INFO:     127.0.0.1:40102 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 09-02 11:09:26 [metrics.py:386] Avg prompt throughput: 7.1 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO 09-02 11:09:26 [metrics.py:402] Prefix cache hit rate: GPU: 0.00%, CPU: 0.00%
[1;36m(APIServer pid=637228)[0;0m INFO 09-02 11:09:28 [launcher.py:101] Shutting down FastAPI HTTP server.
[rank0]:[W902 11:09:29.615438497 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[1;36m(APIServer pid=637228)[0;0m INFO:     Shutting down
[1;36m(APIServer pid=637228)[0;0m INFO:     Waiting for application shutdown.
[1;36m(APIServer pid=637228)[0;0m INFO:     Application shutdown complete.
